{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Nabilah Anuwar\n",
    "    \n",
    "**Student ID:** 31282016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents<a class=\"anchor\" id=\"table\"></a>\n",
    "\n",
    "* [1 Working with RDD](#1)\n",
    "* [1.1 Data Preparation and Loading](#1.1)\n",
    "* [1.1.1 Creating SparkSession & SparkContext](#OneOneOne)\n",
    "* [1.1.2 Read CSV files, Preprocessing, and final(formatted data) RDD for each file](#OneOneTwo)\n",
    "* [1.1.2.1 Flights RDD](#1.1.2.1)\n",
    "* [1.1.2.2 Airports RDD](#1.1.2.2)\n",
    "* [1.1.3 Show RDD number of columns, and number of records](#1.1.3)\n",
    "* [1.2 Dataset flights partitioning](#1.2)\n",
    "* [1.2.1 Obtain the maximum arrival time ](#1.2.1)\n",
    "* [1.2.2 Obtain the maximum minimum time ](#1.2.2)\n",
    "* [1.2.3 Define hash partitioning](#1.2.3)\n",
    "* [1.2.4 Display the records in each partition](#1.2.4)\n",
    "* [1.3 Query RDD](#1.3)\n",
    "* [1.3.1 Collect a total number of flights for each month for all flights](#1.3.1)\n",
    "* [1.3.2 Collect the average delay for each month for all flights](#1.3.2)\n",
    "* [2 Working with DataFrames](#2)\n",
    "* [2.1 Data Preparation and Loading](#2.1)\n",
    "* [2.1.1 Define DataFrames](#2.1.1)\n",
    "* [2.1.2 Display the Scheme of DataFrames](#2.1.2)\n",
    "* [2.1.3 Transform date-time and location column](#2.1.3)\n",
    "* [2.2.1 January Flights Events with ANC airport](#2.2.1)\n",
    "* [2.2.2 Average Arrival Delay From Origin to Destination](#2.2.2)\n",
    "* [2.2.3 Join Query with Airports DataFrame](#2.2.3)\n",
    "* [2.3 Analysis](#2.3.1)\n",
    "* [2.3.1 Relationship between day of week with mean arrival delay, total time delay, and count flights](#2.3.1)\n",
    "* [2.3.2 Display mean arrival delay each month](#2.3.2)\n",
    "* [2.3.3 Relationship between mean departure delay and mean arrival delay](#2.3.3)\n",
    "* [3 RDDs vs DataFrame vs Spark SQL](#3)\n",
    "* [3.1 RDD Operation](#3.1)\n",
    "* [3.2 DataFrame Operation](#3.1)\n",
    "* [3.3 Spark SQL Operation](#3.1)\n",
    "* [3.4 Discussion](#3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.functions import col, when\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Working with RDD<a class=\"anchor\" id=\"1\"></a>\n",
    "## 1.1 Data Preparation and Loading<a class=\"anchor\" id=\"1.1\"></a>\n",
    "### 1.1.1 Create SparkSession and SparkContext<a class=\"anchor\" id=\"OneOneOne\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "master = \"local[*]\"\n",
    "app_name = \"31282016\"\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(conf = spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Import CSV files and Make RDD for each file<a class=\"anchor\" id=\"OneOneTwo\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_rdd0 = sc.textFile(\"flight-delays/flight*.csv\")\n",
    "airports_rdd0 = sc.textFile(\"flight-delays/airports.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.1 Flights RDD <a class=\"anchor\" id=\"1.1.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to convert certain columns to their correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_them(rdd):\n",
    "    new_rdd = []\n",
    "    for i in rdd:\n",
    "        new_row = []\n",
    "        for f in list(range(len(i))):\n",
    "            if i[f] in ('', None, \" \", \"\") and (f in [0,1,2,3,5,11,12,15,16,17,19,22]):\n",
    "                new_row.append(None)\n",
    "            elif f in [0,1,2,3,5]:\n",
    "                new_row.append(int(i[f]))\n",
    "            elif f in [11,12,15,16,17,19,22]:\n",
    "                new_row.append(float(i[f]))\n",
    "            else:\n",
    "                new_row.append(i[f])\n",
    "        new_rdd.append(new_row)\n",
    "    return new_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the words by commas and making it into a list for each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_rdd1 = flights_rdd0.map(lambda line: line.split(','))\n",
    "header_flights = flights_rdd1.first()\n",
    "flights_rdd1 = flights_rdd1.filter(lambda row: row != header_flights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a schema and creating a DataFrame with that schema and our previous rdd. The columns YEAR, MONTH, DAY, DAY_OF_WEEK, and FLIGHT_NUMBER is Integer type based one the question. Columns such as DEPARTURE_DELAY, TAXI_OUT, ELAPSED_TIME, AIR_TIME, DISTANCE, TAXI_IN, and ARRIVAL_DELAY are Float type based on the request of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_list = convert_them(flights_rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_flights = StructType([\n",
    "    StructField(\"YEAR\", IntegerType()),\n",
    "    StructField(\"MONTH\", IntegerType()),\n",
    "    StructField(\"DAY\", IntegerType()),\n",
    "    StructField(\"DAY_OF_WEEK\", IntegerType()),\n",
    "    StructField(\"AIRLINE\", StringType()),\n",
    "    StructField(\"FLIGHT_NUMBER\", IntegerType()),\n",
    "    StructField(\"TAIL_NUMBER\", StringType()),\n",
    "    StructField(\"ORIGIN_AIRPORT\", StringType()),\n",
    "    StructField(\"DESTINATION_AIRPORT\", StringType()),\n",
    "    StructField(\"SCHEDULED_DEPARTURE\", StringType()),\n",
    "    StructField(\"DEPARTURE_TIME\", StringType()),\n",
    "    StructField(\"DEPARTURE_DELAY\", FloatType()),\n",
    "    StructField(\"TAXI_OUT\", FloatType()),\n",
    "    StructField(\"WHEELS_OFF\", StringType()),\n",
    "    StructField(\"SCHEDULED_TIME\", StringType()),\n",
    "    StructField(\"ELAPSED_TIME\", FloatType()),\n",
    "    StructField(\"AIR_TIME\", FloatType()),\n",
    "    StructField(\"DISTANCE\", FloatType()),\n",
    "    StructField(\"WHEELS_ON\", StringType()),\n",
    "    StructField(\"TAXI_IN\", FloatType()),\n",
    "    StructField(\"SCHEDULED_ARRIVAL\", StringType()),\n",
    "    StructField(\"ARRIVAL_TIME\", StringType()),\n",
    "    StructField(\"ARRIVAL_DELAY\", FloatType()),\n",
    "    StructField(\"DIVERTED\", StringType()),\n",
    "    StructField(\"CANCELLED\", StringType()),\n",
    "    StructField(\"CANCELLATION_REASON\", StringType()),\n",
    "    StructField(\"AIR_SYSTEM_DELAY\", StringType()),\n",
    "    StructField(\"SECURITY_DELAY\", StringType()),\n",
    "    StructField(\"AIRLINE_DELAY\", StringType()),\n",
    "    StructField(\"LATE_AIRCRAFT_DELAY\", StringType()),\n",
    "    StructField(\"WEATHER_DELAY\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = spark.createDataFrame(flights_list, schema_flights)\n",
    "flights_rdd = flights_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(YEAR=2015, MONTH=6, DAY=26, DAY_OF_WEEK=5, AIRLINE='EV', FLIGHT_NUMBER=4951, TAIL_NUMBER='N707EV', ORIGIN_AIRPORT='BHM', DESTINATION_AIRPORT='LGA', SCHEDULED_DEPARTURE='630', DEPARTURE_TIME='629', DEPARTURE_DELAY=-1.0, TAXI_OUT=13.0, WHEELS_OFF='642', SCHEDULED_TIME='155', ELAPSED_TIME=141.0, AIR_TIME=113.0, DISTANCE=866.0, WHEELS_ON='935', TAXI_IN=15.0, SCHEDULED_ARRIVAL='1005', ARRIVAL_TIME='950', ARRIVAL_DELAY=-15.0, DIVERTED='0', CANCELLED='0', CANCELLATION_REASON='', AIR_SYSTEM_DELAY='', SECURITY_DELAY='', AIRLINE_DELAY='', LATE_AIRCRAFT_DELAY='', WEATHER_DELAY='')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_rdd.take(1)\n",
    "# 05081995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.2 Airports RDD <a class=\"anchor\" id=\"1.1.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the words by commas and making it into a list for each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_rdd1 = airports_rdd0.map(lambda line: line.split(','))\n",
    "header_airport = airports_rdd1.first()\n",
    "airports_rdd1 = airports_rdd1.filter(lambda row: row != header_airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a schema and make a DataFrame from the `airports_rdd1` then convert it to rdd. There is no specification on the data type thus we make all a String type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_airport = StructType([\n",
    "    StructField(\"IATA_CODE\", StringType()),\n",
    "    StructField(\"AIRPORT\", StringType()),\n",
    "    StructField(\"CITY\", StringType()),\n",
    "    StructField(\"STATE\", StringType()),\n",
    "    StructField(\"COUNTRY\", StringType()),\n",
    "    StructField(\"ALATITUDA\", StringType()),\n",
    "    StructField(\"ALONGITUDE\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = spark.createDataFrame(airports_rdd1, schema_airport)\n",
    "airports_rdd = airports_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(IATA_CODE='ABE', AIRPORT='Lehigh Valley International Airport', CITY='Allentown', STATE='PA', COUNTRY='USA', ALATITUDA='40.65236', ALONGITUDE='-75.44040'),\n",
       " Row(IATA_CODE='ABI', AIRPORT='Abilene Regional Airport', CITY='Abilene', STATE='TX', COUNTRY='USA', ALATITUDA='32.41132', ALONGITUDE='-99.68190')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_rdd.take(2)\n",
    "# 05081995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Show RDD number of columns, and number of records <a class=\"anchor\" id=\"1.1.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns\n",
      "flights_rdd: 31\n",
      "airports_rdd: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Columns\")\n",
    "print(\"flights_rdd:\", flights_rdd.map(lambda x: len(x)).take(1)[0])\n",
    "print(\"airports_rdd:\", airports_rdd.map(lambda x: len(x)).take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Records\n",
      "flights_rdd: 582184\n",
      "airports_rdd: 322\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Records\")\n",
    "trec_flights = flights_rdd.cache().count()\n",
    "trec_airports = airports_rdd.cache().count()\n",
    "print(\"flights_rdd:\", trec_flights)\n",
    "print(\"airports_rdd:\", trec_airports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions\n",
      "flights_rdd: 2\n",
      "airports_rdd: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions\")\n",
    "print(\"flights_rdd:\", flights_rdd.getNumPartitions())\n",
    "print(\"airports_rdd:\", airports_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Partitioning <a class=\"anchor\" id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First we need to check which rows have missing values within `ARRIVAL_DELAY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10455\n"
     ]
    }
   ],
   "source": [
    "print(flights_rdd.filter(lambda x: x[22] == None).cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `ARRIVAL_DELAY` is a result from subtracting `ARRIVAL_TIME` and `SCHEDULED_ARRIVAL`. We want to see which of these is the cause of `ARRIVAL_DELAY` being missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing ARRIVAL_DELAY and ARRIVAL_TIME: 9257\n",
      "Missing ARRIVAL_DELAY and SCHEDULED_ARRIVAL: 0\n"
     ]
    }
   ],
   "source": [
    "mAD_AT = flights_rdd.filter(lambda x: x[22] == None and x[21] == \"\").cache().count()\n",
    "mAD_SA = flights_rdd.filter(lambda x: x[22] == None and x[20] == \"\").cache().count()\n",
    "print(\"Missing ARRIVAL_DELAY and ARRIVAL_TIME:\", mAD_AT)\n",
    "print(\"Missing ARRIVAL_DELAY and SCHEDULED_ARRIVAL:\", mAD_SA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From this, we find that there is a difference in number between point 1 and 2 which means that some values of `ARRIVAL_DELAY` is not calculated properly.\n",
    "\n",
    "* Another finding is that the `SCHEDULED_TIME` seems to not be the cause of the missing value.\n",
    "\n",
    "Before continuing we will need to calculate the missing value from the first finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# function to make sure string length is 4 and add 0s\n",
    "def add_zeros(a):\n",
    "    if len(str(a)) < 4:\n",
    "        zeros = \"0\"*(4-len(str(a)))\n",
    "        result = zeros+str(a)\n",
    "    else:\n",
    "        result = str(a)\n",
    "    return result\n",
    "\n",
    "# get minutes from difference of time\n",
    "def get_mins(a,b):\n",
    "    a = add_zeros(a)\n",
    "    b = add_zeros(b)\n",
    "    ah = int(a[:-2])\n",
    "    am = int(a[-2:])\n",
    "    \n",
    "    bh = int(b[:-2])\n",
    "    bm = int(b[-2:])\n",
    "    \n",
    "    m1 = (ah*60)+am\n",
    "    m2 = (bh*60)+bm\n",
    "    \n",
    "    diff = float(m1-m2)\n",
    "    \n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_rdda = flights_rdd.filter(lambda x: x[22] == None and x[21] != \"\" and x[20] != \"\") # not yet calculated\n",
    "flights_rddb = flights_rdd.filter(lambda x: x[22] == None and x[21] == \"\") # missing ARRIVAL_TIME\n",
    "flights_rddc = flights_rdd.filter(lambda x: x[22] != None) # not missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assume that all the values are appropriate for `flights_rdda`. The values on `ARRIVAL_TIME` and `SCHEDULED_TIME` is HHmm. However, if the hours are not present it will not be written so 0s at the front will not be written. Example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_rdda = flights_rdda.map(lambda x: Row(*x[0:22], get_mins(x[21],x[20]), *x[23:]))\n",
    "flights_rdda = spark.createDataFrame(flights_rdda, schema_flights).rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `ARRIVAL_TIME` is calculated from `WHEELS_ON` and `TAXI_IN`. We can try to find how many from `flights_rddb` that have these two values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(flights_rddb.filter(lambda x: x[18] != \"\" and x[19] != None).cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the result above that there are no data from `flights_rddb` that has valid values of `WHEELS_ON` and `TAXI_IN`. We will keep the `ARRIVAL_DELAY` column as None and remove these row from the maximum and minimum calculation. We will not use the value 0 as it means there is no delay in the flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data that we lost removing flights_rddb: 1.59%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of data that we lost removing flights_rddb: {round((mAD_AT/trec_flights)*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can now combine those with non empty values in `ARRIVAL_DELAY` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "frdd_clean = flights_rdda.union(flights_rddc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Obtain the maximum arrival delay <a class=\"anchor\" id=\"1.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum arrival delay is 1665.0 minutes\n"
     ]
    }
   ],
   "source": [
    "q121 = frdd_clean.max(lambda x: x[22])[22]\n",
    "print(f\"Maximum arrival delay is {q121} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Obtain the minimum arrival delay <a class=\"anchor\" id=\"1.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum arrival delay is -1371.0 minutes\n"
     ]
    }
   ],
   "source": [
    "q122 = frdd_clean.min(lambda x: x[22])[22]\n",
    "print(f\"Minimum arrival delay is {q122} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Define hash partitioning function <a class=\"anchor\" id=\"1.2.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hash Partitioning is not like Range Partitioning. Its main goal is to try to distribute the rows evenly among the partitions. Thus, it is unlikely that max and min can fully determine the distribution within the partition.\n",
    "\n",
    "* make `ARRIVAL_DELAY` the key\n",
    "\n",
    "* get max and min of `ARRIVAL_DELAY` to determine the distribution within the partition by having a range function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get ideal number of partition, the ideal amount of partition is total cores times 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cores = sc.defaultParallelism\n",
    "part_no = total_cores*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of partitions that will be used is: 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of partitions that will be used is: {part_no}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define Hash Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function(key):\n",
    "    print(key)\n",
    "    the_range = []\n",
    "    gaps = (q121-q122)/part_no\n",
    "    low = q122\n",
    "    for i in list(range(part_no)):\n",
    "        the_range.append(low+(gaps*i))\n",
    "    \n",
    "    for r in list(range(part_no)):\n",
    "        if float(key) <= the_range[r]:\n",
    "            return int(the_range[r]-float(key))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create Hash Partitioning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partByhash(the_rdd):\n",
    "    # make ARRIVAL_DELAY the key\n",
    "    data = the_rdd.map(lambda x: (x[22],x))\n",
    "    \n",
    "    result = data.partitionBy(part_no, hash_function)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "frdd_hash = partByhash(frdd_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Display the records in each partition <a class=\"anchor\" id=\"1.2.4\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_partitions(data):\n",
    "    numPartitions = data.getNumPartitions()\n",
    "    partitions = data.glom().collect()\n",
    "    \n",
    "    print(f\"####### NUMBER OF PARTITIONS: {numPartitions}\")\n",
    "    for index, partition in enumerate(partitions):\n",
    "        # show partition if it is not empty\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NUMBER OF PARTITIONS: 8\n",
      "Partition 0: 71783 records\n",
      "Partition 1: 71288 records\n",
      "Partition 2: 71902 records\n",
      "Partition 3: 71740 records\n",
      "Partition 4: 71494 records\n",
      "Partition 5: 71499 records\n",
      "Partition 6: 71520 records\n",
      "Partition 7: 71701 records\n"
     ]
    }
   ],
   "source": [
    "print_partitions(frdd_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the hash function try to distribute the data based on the maximum and minimum values of the `ARRIVAL_DELAY` column. I try to make the function to group them based on the range list that was created then get the integer difference between the number within the range and the key itself.\n",
    "\n",
    "**Effect of number of partitions in processing speed**\n",
    "Number of partition need to be just enough for us to use partition ideally. Too less means we are not using all the available cores, which means we might put too much data in the few partitions where we can give them lighter load. If we have too much partitions, the cores might not be able to parallel process them as there are more partitions than the available cores. Thus, we determine the number of partitions based on the core. Each core can handle around 3 to 4 partitions. Thus the ideal number of partition is the number of cores x 3 or 4, in this case we times it by 4.\n",
    "\n",
    "**Effect of hash functions in processing speed**\n",
    "As mentioned before, hash partitioning main goal is to evenly distribute the data in all partitions. If we use range partitioning it might be useful for time bound data, however I believe this data will not be used mainly from its timed data. Hash partitioning helps by putting even load to all partitions, thus all would have almost the same amount of time to process.\n",
    "\n",
    "**Normal Partitioning vs Hash Partitioning**\n",
    "Though they both aim to partition entries evenly, hash partitioning is better as it mixes the entries. Thus one partition will have different types of entries while normal partition will have similar data types in the partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Query RDD  <a class=\"anchor\" id=\"1.3\"></a>\n",
    "### 1.3.1 Collect a total number of flights for each month <a class=\"anchor\" id=\"1.3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the full uncleaned data as it is more suitable to represent the total number of flights each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 47136),\n",
       " (2, 42798),\n",
       " (3, 50816),\n",
       " (4, 48810),\n",
       " (5, 49691),\n",
       " (6, 50256),\n",
       " (7, 52065),\n",
       " (8, 50524),\n",
       " (9, 46733),\n",
       " (10, 48680),\n",
       " (11, 46809),\n",
       " (12, 47866)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q131 = flights_rdd.map(lambda x: (x[1],1)).groupByKey().mapValues(len).sortByKey()\n",
    "q131.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Collect the average delay for each month <a class=\"anchor\" id=\"1.3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use cleaned data as the unavailable rows does now have valid data type for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5.86),\n",
       " (2, 8.07),\n",
       " (3, 5.09),\n",
       " (4, 3.29),\n",
       " (5, 4.69),\n",
       " (6, 9.95),\n",
       " (7, 6.8),\n",
       " (8, 4.79),\n",
       " (9, -0.84),\n",
       " (10, -0.63),\n",
       " (11, 0.91),\n",
       " (12, 6.07)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "q132 = frdd_clean.map(lambda x: (x[1],x[22])).groupByKey().mapValues(mean).map(lambda x: (x[0],round(x[1],2))).sortByKey()\n",
    "q132.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Working with DataFrame <a class=\"anchor\" id=\"2\"></a>\n",
    "## 2.1. Data Preparation and Loading <a class=\"anchor\" id=\"2.1\"></a>\n",
    "### 2.1.1 Define dataframes and loading scheme<a class=\"anchor\" id=\"2.1.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDf = spark.read.load(\"flight-delays/flight*.csv\", format = \"csv\", sep = \",\", inferSchema = True, header = True)\n",
    "airportsDf = spark.read.load(\"flight-delays/airports.csv\", format = \"csv\", sep = \",\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Display the schema of the final two dataframes<a class=\"anchor\" id=\"2.1.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- AIRPORT: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning from Section 1, we found that `ARRIVAL_DELAY` had some missing values, yet some was possible to be calculated. So we will go through that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nulls without cleaning is 10455 entries\n"
     ]
    }
   ],
   "source": [
    "x = flightsDf.filter(col(\"ARRIVAL_DELAY\").isNull()).count()\n",
    "print(f\"The number of nulls without cleaning is {x} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minutes from difference of time but integer result\n",
    "def get_mins2(a,b):\n",
    "    #different in this part as DF convert the ones with 0s to None rather than as a string\n",
    "    if a == None:\n",
    "        a = \"0\"\n",
    "    if b == None:\n",
    "        b = \"0\"\n",
    "    a = add_zeros(a)\n",
    "    b = add_zeros(b)\n",
    "    ah = int(a[:-2])\n",
    "    am = int(a[-2:])\n",
    "    \n",
    "    bh = int(b[:-2])\n",
    "    bm = int(b[-2:])\n",
    "    \n",
    "    m1 = (ah*60)+am\n",
    "    m2 = (bh*60)+bm\n",
    "    \n",
    "    diff = float(m1-m2)\n",
    "    # the result should be in integer not float\n",
    "    return int(diff)\n",
    "\n",
    "# create UDF\n",
    "a_udf = UserDefinedFunction(get_mins2, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDf = flightsDf.withColumn(\"ARRIVAL_DELAY\", when((col(\"ARRIVAL_DELAY\").isNull()) & (col(\"ARRIVAL_TIME\").isNull() == False) & (col(\"SCHEDULED_TIME\").isNull() == False), a_udf(\"ARRIVAL_TIME\",\"SCHEDULED_TIME\")).otherwise(col(\"ARRIVAL_DELAY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nulls with cleaning is 9257 entries\n"
     ]
    }
   ],
   "source": [
    "y = flightsDf.filter(col(\"ARRIVAL_DELAY\").isNull()).count()\n",
    "print(f\"The number of nulls with cleaning is {y} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers match to our previous finding in RDD section, so we can continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Query Analysis <a class=\"anchor\" id=\"2.2\"></a>\n",
    "### 2.2.1 January flight events with ANC airport <a class=\"anchor\" id=\"2.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-------------------+--------+-------------+\n",
      "|MONTH|ORIGIN_AIRPORT|DESTINATION_AIRPORT|DISTANCE|ARRIVAL_DELAY|\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "|    1|           ANC|                SEA|    1448|          -13|\n",
      "|    1|           ANC|                SEA|    1448|           -4|\n",
      "|    1|           ANC|                JNU|     571|           17|\n",
      "|    1|           ANC|                CDV|     160|           20|\n",
      "|    1|           ANC|                BET|     399|          -20|\n",
      "|    1|           ANC|                SEA|    1448|          -15|\n",
      "|    1|           ANC|                SEA|    1448|          -11|\n",
      "|    1|           ANC|                ADQ|     253|          -16|\n",
      "|    1|           ANC|                SEA|    1448|           17|\n",
      "|    1|           ANC|                BET|     399|           -9|\n",
      "|    1|           ANC|                SEA|    1448|           15|\n",
      "|    1|           ANC|                FAI|     261|           -6|\n",
      "|    1|           ANC|                JNU|     571|            2|\n",
      "|    1|           ANC|                JNU|     571|           -3|\n",
      "|    1|           ANC|                PDX|    1542|          -21|\n",
      "|    1|           ANC|                SEA|    1448|           -5|\n",
      "|    1|           ANC|                SEA|    1448|          -15|\n",
      "|    1|           ANC|                PDX|    1542|          -13|\n",
      "|    1|           ANC|                SFO|    2018|           20|\n",
      "|    1|           ANC|                FAI|     261|           56|\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "janFlightEventsAncDf = flightsDf.filter(col(\"MONTH\")==1).filter(col(\"YEAR\")== 2015).filter(col(\"ORIGIN_AIRPORT\")==\"ANC\").select(\"MONTH\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"DISTANCE\",\"ARRIVAL_DELAY\")\n",
    "janFlightEventsAncDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Average Arrival Delay From Origin to Destination <a class=\"anchor\" id=\"2.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results in `AVERAGE_DELAY` will be rounded to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|AVERAGE_DELAY|\n",
      "+--------------+-------------------+-------------+\n",
      "|           ANC|                ADK|        -27.0|\n",
      "|           ANC|                HNL|        -20.0|\n",
      "|           ANC|                MSP|       -19.25|\n",
      "|           ANC|                BET|        -9.09|\n",
      "|           ANC|                SEA|        -6.49|\n",
      "|           ANC|                BRW|        -4.33|\n",
      "|           ANC|                OME|         -3.0|\n",
      "|           ANC|                ADQ|        -2.67|\n",
      "|           ANC|                CDV|          1.0|\n",
      "|           ANC|                OTZ|         1.25|\n",
      "|           ANC|                PHX|          2.0|\n",
      "|           ANC|                DEN|         3.33|\n",
      "|           ANC|                PDX|          3.5|\n",
      "|           ANC|                JNU|          5.0|\n",
      "|           ANC|                LAS|          9.0|\n",
      "|           ANC|                SCC|        16.67|\n",
      "|           ANC|                SFO|         20.0|\n",
      "|           ANC|                FAI|         25.0|\n",
      "+--------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "janFlightsEventsAncAvgDf = janFlightEventsAncDf.groupBy(col(\"ORIGIN_AIRPORT\"), col(\"DESTINATION_AIRPORT\")).agg(F.round(F.mean(\"ARRIVAL_DELAY\"),2).alias(\"AVERAGE_DELAY\")).orderBy(col(\"AVERAGE_DELAY\"))\n",
    "janFlightsEventsAncAvgDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Join Query with Airports DataFrame <a class=\"anchor\" id=\"2.2.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have `janFlightsEventsAncAvgDf` `ORIGIN_AIRPORT` equal to `airportsDf` `IATA_CODE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------+---------+--------------------+---------+-----+-------+--------+----------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|AVERAGE_DELAY|IATA_CODE|             AIRPORT|     CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+--------------+-------------------+-------------+---------+--------------------+---------+-----+-------+--------+----------+\n",
      "|           ANC|                BRW|        -4.33|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                ADK|        -27.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                OME|         -3.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                JNU|          5.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                LAS|          9.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                SCC|        16.67|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                CDV|          1.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                DEN|         3.33|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                OTZ|         1.25|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                SFO|         20.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                FAI|         25.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                ADQ|        -2.67|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                PDX|          3.5|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                PHX|          2.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                HNL|        -20.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                SEA|        -6.49|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                MSP|       -19.25|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                BET|        -9.09|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "+--------------+-------------------+-------------+---------+--------------------+---------+-----+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedSqlDf = janFlightsEventsAncAvgDf.join(airportsDf, airportsDf.IATA_CODE == janFlightsEventsAncAvgDf.ORIGIN_AIRPORT, how = \"inner\")\n",
    "joinedSqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Analysis <a class=\"anchor\" id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Relationship between day of week with mean arrival delay, total time delay, and count flights <a class=\"anchor\" id=\"2.3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+--------------+------------+\n",
      "|DAY_OF_WEEK| MeanArrivalDelay|TotalTimeDelay|NumOfFlights|\n",
      "+-----------+-----------------+--------------+------------+\n",
      "|          4|7.312340849113385|        631757|       87683|\n",
      "|          1|7.586891745907381|        639097|       86317|\n",
      "|          5|6.328193109913288|        540048|       86253|\n",
      "|          3|5.640166175478464|        476532|       85607|\n",
      "|          2|6.234578920705377|        516884|       84449|\n",
      "|          7|5.993143328171055|        479859|       81422|\n",
      "|          6|3.754716438099898|        260919|       70453|\n",
      "+-----------+-----------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDf.createOrReplaceTempView(\"sql_flights\")\n",
    "q231 = spark.sql('''\n",
    "    SELECT DAY_OF_WEEK, AVG(ARRIVAL_DELAY) AS MeanArrivalDelay, SUM(ARRIVAL_DELAY) AS TotalTimeDelay, COUNT(*) AS NumOfFlights\n",
    "    FROM sql_flights f\n",
    "    WHERE YEAR = 2015\n",
    "    GROUP BY DAY_OF_WEEK\n",
    "    ORDER BY NumOfFlights DESC\n",
    "    ''')\n",
    "q231.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we find that the fourth day of the week or Thursday is when the highest `NumOfFlights` happen. Interestingly, though Thursday have the most flights, Monday or 1st day of the week is the one that have the highest `TotalTimeDelay` and with it having the highest `MeanArrivalDelay`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Display mean arrival delay each month <a class=\"anchor\" id=\"2.3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No requirement showing if we need to find about those especially in the year 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------+------------+\n",
      "|MONTH|  MeanArrivalDelay|TotalTimeDelay|NumOfFlights|\n",
      "+-----+------------------+--------------+------------+\n",
      "|    9| 0.393758328676439|         18320|       46733|\n",
      "|   10|0.5218585441404233|         25271|       48680|\n",
      "|   11|1.9537055047656098|         90396|       46809|\n",
      "|    4|  5.18668404609687|        250688|       48810|\n",
      "|    3| 6.196238345516422|        307699|       50816|\n",
      "|    1| 6.754906653901388|        310442|       47136|\n",
      "|    5| 7.012317025998087|        344438|       49691|\n",
      "|    8| 7.126482292479053|        356374|       50524|\n",
      "|   12| 7.848228338083287|        369008|       47866|\n",
      "|    7| 9.019146459747818|        464937|       52065|\n",
      "|    2| 9.404563857195436|        383283|       42798|\n",
      "|    6|12.660014602092966|        624240|       50256|\n",
      "+-----+------------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q232 = spark.sql('''\n",
    "    SELECT MONTH, AVG(ARRIVAL_DELAY) AS MeanArrivalDelay, SUM(ARRIVAL_DELAY) AS TotalTimeDelay, COUNT(*) AS NumOfFlights\n",
    "    FROM sql_flights f\n",
    "    GROUP BY MONTH\n",
    "    ORDER BY MeanArrivalDelay ASC\n",
    "    ''')\n",
    "q232.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 9th month or September have the lowest `MeanArrivalDelay` compares to other months. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Relationship between mean departure delay and mean arrival delay <a class=\"anchor\" id=\"2.3.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+\n",
      "|MONTH|     MeanDeptDelay|  MeanArrivalDelay|\n",
      "+-----+------------------+------------------+\n",
      "|    6|  13.9730063585922|12.660014602092966|\n",
      "|   12|11.821651454043728| 7.848228338083287|\n",
      "|    7|11.708608758020432| 9.019146459747818|\n",
      "|    2|11.620796080832823| 9.404563857195436|\n",
      "|    8|10.086906141367324| 7.126482292479053|\n",
      "|    1|  9.75401499511029| 6.754906653901388|\n",
      "|    3| 9.718308159530178| 6.196238345516422|\n",
      "|    5| 9.550310180006102| 7.012317025998087|\n",
      "|    4| 7.737554783759199|  5.18668404609687|\n",
      "|   11| 6.630585898709037|1.9537055047656098|\n",
      "|   10| 5.243436261558784|0.5218585441404233|\n",
      "|    9| 4.728506981740065| 0.393758328676439|\n",
      "+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q233 = spark.sql('''\n",
    "    SELECT MONTH, AVG(DEPARTURE_DELAY) AS MeanDeptDelay, AVG(ARRIVAL_DELAY) AS MeanArrivalDelay\n",
    "    FROM sql_flights f\n",
    "    GROUP BY MONTH\n",
    "    ORDER BY MeanDeptDelay DESC\n",
    "    ''')\n",
    "q233.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `MeanDeptDelay` decrease the `MeanArrivalDelay` would decrease as well. However, this theory is true if we exclude the 12th and 5th month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 RDDs vs DataFrame vs Spark SQL <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "\n",
    "Implement the following queries using RDDs, DataFrames and SparkSQL separately. Log the time taken for each query in each approach using the “%%time” built-in magic command in Jupyter Notebook and discuss the performance difference of these 3 approaches.\n",
    "\n",
    "<strong>Find the MONTH and DAY_OF_WEEK, number of flights, and average delay where TAIL_NUMBER = ‘N407AS’. Note number of flights and average delay should be aggregated separately. The average delay should be grouped by both MONTH and DAYS_OF_WEEK.</strong>\n",
    "\n",
    "## 3.1 RDD Operation<a class=\"anchor\" id=\"3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.3 ns ± 0.947 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "q31 = frdd_clean.filter(lambda x: x[\"TAIL_NUMBER\"] == \"N407AS\")\\\n",
    ".map(lambda x: ((x[\"MONTH\"], x[\"DAY_OF_WEEK\"]),(1, x[\"DEPARTURE_DELAY\"],x[\"ARRIVAL_DELAY\"])))\\\n",
    ".reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2]+y[2]))\\\n",
    ".mapValues(lambda x: (x[0],round(x[1]/x[0],2),round(x[2]/x[0],2)))\\\n",
    ".sortByKey()\n",
    "%timeit q31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), (1, 4.0, -6.0)),\n",
       " ((1, 2), (2, 12.5, 17.5)),\n",
       " ((1, 3), (1, -7.0, -27.0)),\n",
       " ((1, 5), (2, -6.0, -21.0)),\n",
       " ((1, 6), (3, 8.67, 4.33)),\n",
       " ((2, 1), (2, -4.0, -2.5)),\n",
       " ((2, 2), (2, -3.5, -9.5)),\n",
       " ((2, 3), (2, -12.5, -11.5)),\n",
       " ((2, 4), (2, -8.5, -11.0)),\n",
       " ((2, 5), (1, -11.0, -31.0)),\n",
       " ((2, 7), (2, -7.0, 6.5)),\n",
       " ((3, 1), (1, 40.0, 29.0)),\n",
       " ((3, 2), (2, -5.5, -28.0)),\n",
       " ((3, 3), (1, 28.0, 3.0)),\n",
       " ((3, 4), (1, 1.0, 2.0)),\n",
       " ((3, 5), (3, 5.67, 6.67)),\n",
       " ((3, 6), (1, -1.0, -3.0)),\n",
       " ((4, 1), (1, -1.0, 0.0)),\n",
       " ((4, 2), (1, -2.0, 6.0)),\n",
       " ((4, 3), (1, -4.0, -7.0))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q31.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DataFrame Operation<a class=\"anchor\" id=\"3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.1 ns ± 0.337 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "q32 = flightsDf.filter(col(\"TAIL_NUMBER\")==\"N407AS\")\\\n",
    ".groupBy(col(\"MONTH\"),col(\"DAY_OF_WEEK\"))\\\n",
    ".agg(F.count(\"MONTH\").alias(\"NumOfFlights\"), F.round(F.mean(\"DEPARTURE_DELAY\"),2).alias(\"MeanDeptDelay\"), F.round(F.mean(\"ARRIVAL_DELAY\"),2).alias(\"MeanArrivalDelay\"))\\\n",
    ".orderBy(\"MONTH\",\"DAY_OF_WEEK\")\n",
    "%timeit q32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------+-------------+----------------+\n",
      "|MONTH|DAY_OF_WEEK|NumOfFlights|MeanDeptDelay|MeanArrivalDelay|\n",
      "+-----+-----------+------------+-------------+----------------+\n",
      "|    1|          1|           1|          4.0|            -6.0|\n",
      "|    1|          2|           2|         12.5|            17.5|\n",
      "|    1|          3|           1|         -7.0|           -27.0|\n",
      "|    1|          5|           2|         -6.0|           -21.0|\n",
      "|    1|          6|           3|         8.67|            4.33|\n",
      "|    2|          1|           2|         -4.0|            -2.5|\n",
      "|    2|          2|           2|         -3.5|            -9.5|\n",
      "|    2|          3|           2|        -12.5|           -11.5|\n",
      "|    2|          4|           2|         -8.5|           -11.0|\n",
      "|    2|          5|           1|        -11.0|           -31.0|\n",
      "|    2|          7|           2|         -7.0|             6.5|\n",
      "|    3|          1|           1|         40.0|            29.0|\n",
      "|    3|          2|           2|         -5.5|           -28.0|\n",
      "|    3|          3|           1|         28.0|             3.0|\n",
      "|    3|          4|           1|          1.0|             2.0|\n",
      "|    3|          5|           3|         5.67|            6.67|\n",
      "|    3|          6|           1|         -1.0|            -3.0|\n",
      "|    4|          1|           1|         -1.0|             0.0|\n",
      "|    4|          2|           1|         -2.0|             6.0|\n",
      "|    4|          3|           1|         -4.0|            -7.0|\n",
      "+-----+-----------+------------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q32.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Spark SQL OPERATION<a class=\"anchor\" id=\"3.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.3 ns ± 0.448 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "q33 = spark.sql('''\n",
    "    SELECT MONTH, DAY_OF_WEEK, COUNT(*) AS NumOfFlights, AVG(DEPARTURE_DELAY) AS MeanDeptDelay, AVG(ARRIVAL_DELAY) AS MeanArrivalDelay\n",
    "    FROM sql_flights f\n",
    "    WHERE TAIL_NUMBER == \"N407AS\"\n",
    "    GROUP BY MONTH, DAY_OF_WEEK\n",
    "    ORDER BY MONTH, DAY_OF_WEEK\n",
    "    ''')\n",
    "%timeit q33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------+-----------------+-----------------+\n",
      "|MONTH|DAY_OF_WEEK|NumOfFlights|    MeanDeptDelay| MeanArrivalDelay|\n",
      "+-----+-----------+------------+-----------------+-----------------+\n",
      "|    1|          1|           1|              4.0|             -6.0|\n",
      "|    1|          2|           2|             12.5|             17.5|\n",
      "|    1|          3|           1|             -7.0|            -27.0|\n",
      "|    1|          5|           2|             -6.0|            -21.0|\n",
      "|    1|          6|           3|8.666666666666666|4.333333333333333|\n",
      "|    2|          1|           2|             -4.0|             -2.5|\n",
      "|    2|          2|           2|             -3.5|             -9.5|\n",
      "|    2|          3|           2|            -12.5|            -11.5|\n",
      "|    2|          4|           2|             -8.5|            -11.0|\n",
      "|    2|          5|           1|            -11.0|            -31.0|\n",
      "|    2|          7|           2|             -7.0|              6.5|\n",
      "|    3|          1|           1|             40.0|             29.0|\n",
      "|    3|          2|           2|             -5.5|            -28.0|\n",
      "|    3|          3|           1|             28.0|              3.0|\n",
      "|    3|          4|           1|              1.0|              2.0|\n",
      "|    3|          5|           3|5.666666666666667|6.666666666666667|\n",
      "|    3|          6|           1|             -1.0|             -3.0|\n",
      "|    4|          1|           1|             -1.0|              0.0|\n",
      "|    4|          2|           1|             -2.0|              6.0|\n",
      "|    4|          3|           1|             -4.0|             -7.0|\n",
      "+-----+-----------+------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q33.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Discussion<a class=\"anchor\" id=\"3.4\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the queries above we can see that RDD query have the fastest result. This is because RDD is considered raw. It has not been processed fully like DataFrame, which makes it easier for computers to process, though not the same for humans. \n",
    "\n",
    "SQL and DataFrame has a similar time result. This may be because SQL was derived from the DataFrame. But it is a query directly towards the data. For DataFrame, they are already processed and its more visualised. When we query with DataFrame we are trying to query from the table not the data itself. Which explains the time difference.\n",
    "\n",
    "RDD may also be faster since we only preprocessed necessary columns while DataFrame and SQL set everything up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5202 Assignment 1 SOLUTION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
